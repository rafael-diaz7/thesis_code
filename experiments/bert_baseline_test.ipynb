{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T01:28:24.334897Z",
     "start_time": "2024-06-04T01:28:18.688249Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from experiments.Models import BertBaseline\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 21:28:21.414054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-03 21:28:21.414386: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-03 21:28:21.508968: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-03 21:28:21.705111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 21:28:22.928262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# model = BertBaseline()\n",
    "# model.load_model_weights('../models/bert_evidence_baseline_weights.h5')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "test_dataset = pd.read_csv('../data/emrqa_evidence_test.csv')\n",
    "tokenized_test = tokenizer(test_dataset['question'].to_list(),\n",
    "                           test_dataset['evidence'].to_list(),\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=512,\n",
    "                           return_tensors='tf')\n",
    "test_x = (tokenized_test['input_ids'], tokenized_test['attention_mask'])\n",
    "test_y = np.stack((np.eye(512)[test_dataset['start_token']], np.eye(512)[test_dataset['end_token']]), axis=1)\n",
    "results = model.model.predict(test_x, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T01:30:44.388161Z",
     "start_time": "2024-06-04T01:28:53.992912Z"
    }
   },
   "id": "5fd68a50e5208e7b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 21:28:55.940661: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.216452: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.216666: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.217511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.217660: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.217792: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.300429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.300589: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.300732: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-03 21:28:56.300840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10014 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:09:00.0, compute capability: 8.9\n",
      "2024-06-03 21:28:56.747246: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1083102   ['input_ids[0][0]',           \n",
      " )                           ngAndCrossAttentions(last_   72         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, None,                                            \n",
      "                             768),                                                                \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 2)              1538      ['tf_bert_model[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108311810 (413.18 MB)\n",
      "Trainable params: 108311810 (413.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "207/207 [==============================] - 102s 488ms/step\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:32:47.306632Z",
     "start_time": "2024-06-04T01:32:47.304723Z"
    }
   },
   "cell_type": "code",
   "source": "import tensorflow as tf",
   "id": "6fb8cc81782839a7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:45:55.626831Z",
     "start_time": "2024-06-04T01:45:53.792255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertBaseline()\n",
    "checkpoint = tf.train.Checkpoint(model=model.model)\n",
    "checkpoint.restore('../models/latest/bert_evidence_model')"
   ],
   "id": "eefb860115a389f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertMod  TFBaseModelOutputWithPooli   1083102   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   72         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, None,                                            \n",
      "                             768),                                                                \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, None, 2)              1538      ['tf_bert_model_2[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108311810 (413.18 MB)\n",
      "Trainable params: 108311810 (413.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7d025d5a3190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:48:12.252958Z",
     "start_time": "2024-06-04T01:48:12.250896Z"
    }
   },
   "cell_type": "code",
   "source": "from get_results import compute_metrics, get_prediction",
   "id": "ebd30462ba40c757",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T01:49:50.152747Z",
     "start_time": "2024-06-04T01:48:12.645454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../data/emrqa_evidence_test.csv')\n",
    "tokenized_data = tokenizer(df['question'].to_list(),\n",
    "                           df['evidence'].to_list(),\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=512,\n",
    "                           return_tensors='tf')\n",
    "predictions = model.model.predict((tokenized_data['input_ids'], tokenized_data['attention_mask']))\n",
    "starts = []\n",
    "ends = []\n",
    "for i in predictions:\n",
    "    i = i.T\n",
    "    starts.append(np.argmax(i[0]))\n",
    "    ends.append(np.argmax(i[1]))\n",
    "predictions = pd.DataFrame({\"start\": starts, \"end\": ends})\n",
    "df['predicted_start'], df['predicted_end'] = starts, ends\n",
    "df['predictions'] = df.apply(\n",
    "    lambda x: get_prediction(x['evidence'], x['predicted_start'], x['predicted_end']), axis=1)\n",
    "df['exact_match'], df['precision'], df['recall'], df['f1_score'] = zip(\n",
    "    *df.apply(lambda x: compute_metrics(x['predictions'], x['answer']), axis=1))\n",
    "# get average metrics\n",
    "print(df[['exact_match', 'precision', 'recall', 'f1_score']].mean())"
   ],
   "id": "5bda28550e89c243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 96s 462ms/step\n",
      "exact_match    0.000000\n",
      "precision      0.143941\n",
      "recall         0.030434\n",
      "f1_score       0.045844\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:51:12.371567Z",
     "start_time": "2024-06-04T02:49:32.122479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertBaseline()\n",
    "df = pd.read_csv('../data/emrqa_evidence_test.csv')\n",
    "tokenized_data = tokenizer(df['question'].to_list(),\n",
    "                           df['evidence'].to_list(),\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=512,\n",
    "                           return_tensors='tf')\n",
    "predictions = model.model.predict((tokenized_data['input_ids'], tokenized_data['attention_mask']))\n",
    "starts = []\n",
    "ends = []\n",
    "for i in predictions:\n",
    "    i = i.T\n",
    "    starts.append(np.argmax(i[0]))\n",
    "    ends.append(np.argmax(i[1]))\n",
    "predictions = pd.DataFrame({\"start\": starts, \"end\": ends})\n",
    "df['predicted_start'], df['predicted_end'] = starts, ends\n",
    "df['predictions'] = df.apply(\n",
    "    lambda x: get_prediction(x['evidence'], x['predicted_start'], x['predicted_end']), axis=1)\n",
    "df['exact_match'], df['precision'], df['recall'], df['f1_score'] = zip(\n",
    "    *df.apply(lambda x: compute_metrics(x['predictions'], x['answer']), axis=1))\n",
    "# get average metrics\n",
    "print(df[['exact_match', 'precision', 'recall', 'f1_score']].mean())"
   ],
   "id": "cada09229bba1122",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertMod  TFBaseModelOutputWithPooli   1083102   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   72         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, None,                                            \n",
      "                             768),                                                                \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, None, 2)              1538      ['tf_bert_model_4[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108311810 (413.18 MB)\n",
      "Trainable params: 108311810 (413.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "207/207 [==============================] - 97s 461ms/step\n",
      "exact_match    0.000000\n",
      "precision      0.071668\n",
      "recall         0.016492\n",
      "f1_score       0.024193\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:52:55.462620Z",
     "start_time": "2024-06-04T02:51:12.372524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.saved_model.load('../models/latest/bert_evidence_model')\n",
    "batch_size = 16  # Adjust based on your GPU memory capacity\n",
    "starts, ends = [], []\n",
    "for start in range(0, len(df), batch_size):\n",
    "    end = min(start + batch_size, len(df))\n",
    "    batch_df = df[start:end]\n",
    "\n",
    "    # Tokenize batch data\n",
    "    tokenized_data = tokenizer(\n",
    "        batch_df['question'].to_list(),\n",
    "        batch_df['evidence'].to_list(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    input_data = {\n",
    "        'input_ids': tokenized_data['input_ids'],\n",
    "        'attention_mask': tokenized_data['attention_mask'],\n",
    "    }\n",
    "\n",
    "    inference_func = model.signatures[\"serving_default\"](**input_data)\n",
    "    \n",
    "    for output in inference_func.values():\n",
    "        predictions = output.numpy()\n",
    "        # predictions shape: (batch_size, sequence_length, 2)\n",
    "        start_logits = predictions[:, :, 0]\n",
    "        end_logits = predictions[:, :, 1]\n",
    "        starts.extend(np.argmax(start_logits, axis=1))\n",
    "        ends.extend(np.argmax(end_logits, axis=1))\n",
    "\n",
    "    # start_logits, end_logits = predictions\n",
    "    # starts = np.argmax(start_logits, axis=-1)\n",
    "    # ends = np.argmax(end_logits, axis=-1)\n",
    "\n",
    "print(len(starts), len(ends))\n",
    "df['predicted_start'] = starts\n",
    "df['predicted_end'] = ends\n",
    "df['predictions'] = df.apply(\n",
    "    lambda x: get_prediction(x['evidence'], x['predicted_start'], x['predicted_end']), axis=1)\n",
    "df['exact_match'], df['precision'], df['recall'], df['f1_score'] = zip(\n",
    "    *df.apply(lambda x: compute_metrics(x['predictions'], x['answer']), axis=1))\n",
    "# get average metrics\n",
    "print(df[['exact_match', 'precision', 'recall', 'f1_score']].mean())"
   ],
   "id": "b079628ac587aa3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6601 6601\n",
      "exact_match    0.000303\n",
      "precision      0.026768\n",
      "recall         0.021235\n",
      "f1_score       0.021075\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "starts = []\n",
    "ends = []\n",
    "for i in results:\n",
    "    i = i.T\n",
    "    starts.append(np.argmax(i[0]))\n",
    "    ends.append(np.argmax(i[1]))\n",
    "predictions = pd.DataFrame({\"start\": starts, \"end\": ends})\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T01:30:45.417278Z",
     "start_time": "2024-06-04T01:30:45.366487Z"
    }
   },
   "id": "3dae7c35ebdfe82d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      start  end\n",
       "0       218  110\n",
       "1        23  130\n",
       "2        33  111\n",
       "3       323  443\n",
       "4         6  414\n",
       "...     ...  ...\n",
       "6596     17  440\n",
       "6597    511  143\n",
       "6598    447  425\n",
       "6599     21  237\n",
       "6600     15  496\n",
       "\n",
       "[6601 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>323</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6596</th>\n",
       "      <td>17</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6597</th>\n",
       "      <td>511</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6598</th>\n",
       "      <td>447</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6599</th>\n",
       "      <td>21</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600</th>\n",
       "      <td>15</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6601 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      start  end\n0        25   27\n1        40   40\n2        20   23\n3        33   38\n4        10   13\n...     ...  ...\n6596     30   32\n6597     15   16\n6598     12   24\n6599     22   27\n6600     21   22\n\n[6601 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>40</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6596</th>\n      <td>30</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>6597</th>\n      <td>15</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>6598</th>\n      <td>12</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>6599</th>\n      <td>22</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>6600</th>\n      <td>21</td>\n      <td>22</td>\n    </tr>\n  </tbody>\n</table>\n<p>6601 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_truth = []\n",
    "ends_truth = []\n",
    "for i in test_y:\n",
    "    starts_truth.append(np.argmax(i[0]))\n",
    "    ends_truth.append(np.argmax(i[1]))\n",
    "truth = pd.DataFrame({\"start\": starts_truth, \"end\": ends_truth})\n",
    "truth"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:06:24.763384Z",
     "start_time": "2024-03-26T00:06:24.717440Z"
    }
   },
   "id": "d00afd8363443d7d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "incorrect = 0\n",
    "for ind, i in enumerate(truth):\n",
    "    t = truth.iloc[ind].to_numpy()\n",
    "    p = predictions.iloc[ind].to_numpy()\n",
    "    if not (sum(t == p) == 2):\n",
    "        incorrect += 1\n",
    "print(incorrect)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:06:24.767244Z",
     "start_time": "2024-03-26T00:06:24.764490Z"
    }
   },
   "id": "a2ab91c21591c9dd",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t == p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T23:46:22.371759Z",
     "start_time": "2024-03-25T23:46:22.366727Z"
    }
   },
   "id": "34cbfba2c6a830cf",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "35dc2d691697ba4f",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
